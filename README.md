# llama_pretrain

A lot of the code was inspired from publicly available sources like Huggingface. The code can be used to train any model with a given parameter size.
